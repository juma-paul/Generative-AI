{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Week 11 Homework 2: Generating Answers: Input Text ==> Chunking ==> Embedding ==> Search Index ==> Query ==> Search ==> Question ==> Answer"
      ],
      "metadata": {
        "id": "Zwus3g6JmPwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "vvcXfp7DBEgN"
      },
      "outputs": [],
      "source": [
        "question = \"What are the main vector database providers in the Generative AI?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Generative AI: Comprehensive Landscape Overview\n",
        "\n",
        "Core Concepts\n",
        "Foundational Technologies\n",
        "Large Language Models (LLMs): AI systems trained on massive text datasets to generate human-like text.\n",
        "Transformer Architecture: Neural network design enabling advanced language processing tasks.\n",
        "Generative AI: AI systems capable of creating new content across various formats (text, images, audio, video).\n",
        "\n",
        "Model Types\n",
        "Text Generation Models\n",
        "GPT (OpenAI): Popular model for natural language generation and completion.\n",
        "Claude (Anthropic): Ethical AI-focused LLM for dialogue and reasoning.\n",
        "LLaMA (Meta): Open-source LLM for research and commercial use.\n",
        "Gemini (Google): Google's flagship language model for enterprise use.\n",
        "Mistral: Lightweight and efficient models for diverse applications.\n",
        "Image Generation Models\n",
        "DALL-E (OpenAI): Generates images from textual descriptions.\n",
        "Midjourney: Specialized in high-quality artistic and realistic image generation.\n",
        "Stable Diffusion: Open-source model for customizable image generation.\n",
        "Imagen (Google): High-resolution text-to-image generation model.\n",
        "Multimodal Models\n",
        "GPT-4: Combines text and image inputs for versatile AI applications.\n",
        "Claude 3: Enhances multimodal understanding for complex tasks.\n",
        "Gemini Pro: Designed for robust multimodal use cases in enterprise AI.\n",
        "\n",
        "Key Libraries & Frameworks\n",
        "Python-Based AI Development\n",
        "LangChain: Framework for chaining LLM calls and developing AI applications.\n",
        "Transformers (Hugging Face): Comprehensive library for working with pre-trained LLMs.\n",
        "PyTorch: Flexible and widely-used deep learning framework.\n",
        "TensorFlow: Google's machine learning platform for scalable model development.\n",
        "Model Deployment\n",
        "Ollama: Enables local model hosting and running.\n",
        "Hugging Face: Centralized platform for hosting and sharing models.\n",
        "MLflow: Streamlines the machine learning lifecycle, including tracking and deployment.\n",
        "Inference Platforms\n",
        "OpenAI API: Commercial access to advanced generative AI models.\n",
        "Groq: High-speed inference platform for large models.\n",
        "Anthropic API: Focused on ethical AI deployment.\n",
        "Google Vertex AI: Comprehensive AI deployment for enterprises.\n",
        "\n",
        "Specialized Tools\n",
        "Development Tools\n",
        "Weights & Biases: Tracks ML experiments and manages model versions.\n",
        "Colab: Cloud-based interactive notebook for experimentation.\n",
        "Kaggle: Platform for ML competitions and dataset sharing.\n",
        "Fine-Tuning Tools\n",
        "LoRA: Lightweight fine-tuning for LLMs using low-rank adaptation.\n",
        "Keras: Simplifies neural network fine-tuning.\n",
        "Hugging Face Accelerate: Optimizes distributed training for large models.\n",
        "\n",
        "Data Storage and Retrieval Tools\n",
        "Vector Databases & Semantic Search\n",
        "Weaviate: Open-source vector database for storing and retrieving embeddings, supporting RAG workflows.\n",
        "Features: Hybrid search, cloud-native design, multiple ML model support.\n",
        "Pinecone: Managed vector database offering enterprise scalability and low-latency search.\n",
        "Chroma: Open-source embedding database for ML applications and local storage.\n",
        "Graph Databases\n",
        "Neo4j: Advanced graph database for knowledge graphs, semantic search, and relationship modeling.\n",
        "Use Case: Complex contextual understanding for generative AI.\n",
        "Complementary Tools\n",
        "Qdrant: High-performance vector similarity search engine.\n",
        "Milvus: Scalable open-source vector database with multiple index types.\n",
        "\n",
        "Architectural Integration\n",
        "Pipeline Example:\n",
        "Data Ingestion: Load documents into vector databases like Weaviate.\n",
        "Embedding Creation: Convert text into vector representations.\n",
        "Storage: Save embeddings in a vector database.\n",
        "Query: Use semantic search for context retrieval.\n",
        "Generation: LLM produces a response based on retrieved context.\n",
        "\n",
        "Practical Application Domains\n",
        "Natural Language Processing (NLP): Language understanding, summarization, translation.\n",
        "Computer Vision: Image classification, generation, and object detection.\n",
        "Code Generation: Auto-generating code for programming tasks.\n",
        "Creative Content Production: Generating art, music, or stories.\n",
        "Scientific Research: Data analysis and hypothesis testing.\n",
        "Conversational AI: Chatbots and virtual assistants.\n",
        "Personalized Recommendations: Customized suggestions for users.\n",
        "\n",
        "Challenges in Generative AI\n",
        "Computational Cost: Requires significant resources for training and inference.\n",
        "Bias Mitigation: Addressing inherent biases in datasets.\n",
        "Privacy Concerns: Protecting sensitive data in AI workflows.\n",
        "Ethical Use: Ensuring responsible AI deployment.\n",
        "Hallucination Prevention: Reducing instances of generating incorrect or nonsensical outputs.\n",
        "\n",
        "Emerging Trends\n",
        "Retrieval-Augmented Generation (RAG): Enhances LLMs with external knowledge.\n",
        "Ethical AI Development: Focused on transparency and fairness.\n",
        "Multimodal AI Integration: Combining text, images, and video for holistic AI solutions.\n",
        "Edge AI: Running models locally for privacy and low latency.\n",
        "\n",
        "Future Outlook\n",
        "Increased Model Efficiency: Reducing computational requirements.\n",
        "Specialized Domain Models: Tailored solutions for specific industries.\n",
        "Enhanced Interpretability: Making AI decisions more transparent.\n",
        "Stronger Ethical Frameworks: Ensuring responsible use of AI.\n",
        "Democratization of AI Technologies: Making AI tools accessible to more users.\n",
        "\n",
        "Key Platforms: LLaMA Index and Databricks\n",
        "LLaMA Index (formerly GPT Index)\n",
        "Purpose: Bridges custom data sources with LLMs.\n",
        "Features:\n",
        "Data ingestion from diverse formats (PDFs, APIs, web content).\n",
        "Retrieval-Augmented Generation (RAG) for context-aware responses.\n",
        "Use Cases: Custom AI chatbots, Q&A systems, and contextual applications.\n",
        "Databricks\n",
        "Purpose: Enterprise-grade platform for big data and AI.\n",
        "Components:\n",
        "Databricks Runtime: Spark-based large-scale data processing.\n",
        "Delta Lake: Reliable data storage with ACID compliance.\n",
        "MLflow: Manages the ML lifecycle.\n",
        "Use Cases: Predictive analytics, machine learning, and enterprise AI deployments.\n",
        "\n",
        "Comparative Ecosystem Strengths\n",
        "Tool/Platform\n",
        "Strengths\n",
        "Weaviate\n",
        "Flexibility, open-source\n",
        "Pinecone\n",
        "Scalability, enterprise-grade\n",
        "Neo4j\n",
        "Advanced relationship modeling\n",
        "Chroma\n",
        "ML workflow integration\n",
        "Databricks\n",
        "Big data and enterprise AI support\n",
        "LLaMA Index\n",
        "Simplified AI data connectivity\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FdDtBW3xBcKf"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SETUP**"
      ],
      "metadata": {
        "id": "-gh1AxPGI9nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere python-dotenv annoy\n",
        "import os\n",
        "import cohere\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from annoy import AnnoyIndex\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive (only necessary if working in Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load environment variables from the specified .env file\n",
        "load_dotenv('/content/drive/My Drive/key.env.txt')  # Update the path to your .env file\n",
        "\n",
        "# Access the OpenAI API key\n",
        "OPENAI_API = os.getenv('openai_api_key')\n",
        "COHERE_API = os.getenv('cohere_api_key')"
      ],
      "metadata": {
        "id": "iqKeJp92Biij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7d32da1b-2ded-441d-e9aa-85e45d62a58e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (5.13.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.10/dist-packages (1.17.3)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.7)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.2)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.9.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.20.3)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chunking**"
      ],
      "metadata": {
        "id": "rYMvKjlCM8mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into a list of paragraphs\n",
        "texts = text.split('\\n\\n')\n",
        "\n",
        "# Clean up to remove empty spaces and new lines\n",
        "texts = np.array([t.strip(' \\n') for t in texts if t])"
      ],
      "metadata": {
        "id": "DfH9EQHYCG74"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9TK09QIcCT0L",
        "outputId": "d5dc8a91-6c2b-431b-a8ee-6553f2008710"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Generative AI: Comprehensive Landscape Overview',\n",
              "       'Core Concepts\\nFoundational Technologies\\nLarge Language Models (LLMs): AI systems trained on massive text datasets to generate human-like text.\\nTransformer Architecture: Neural network design enabling advanced language processing tasks.\\nGenerative AI: AI systems capable of creating new content across various formats (text, images, audio, video).',\n",
              "       \"Model Types\\nText Generation Models\\nGPT (OpenAI): Popular model for natural language generation and completion.\\nClaude (Anthropic): Ethical AI-focused LLM for dialogue and reasoning.\\nLLaMA (Meta): Open-source LLM for research and commercial use.\\nGemini (Google): Google's flagship language model for enterprise use.\\nMistral: Lightweight and efficient models for diverse applications.\\nImage Generation Models\\nDALL-E (OpenAI): Generates images from textual descriptions.\\nMidjourney: Specialized in high-quality artistic and realistic image generation.\\nStable Diffusion: Open-source model for customizable image generation.\\nImagen (Google): High-resolution text-to-image generation model.\\nMultimodal Models\\nGPT-4: Combines text and image inputs for versatile AI applications.\\nClaude 3: Enhances multimodal understanding for complex tasks.\\nGemini Pro: Designed for robust multimodal use cases in enterprise AI.\"],\n",
              "      dtype='<U905')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding**"
      ],
      "metadata": {
        "id": "JljYPV35NCa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.Client(COHERE_API)\n",
        "# Get the embeddings\n",
        "response = co.embed(\n",
        "    texts=texts.tolist(),\n",
        ").embeddings"
      ],
      "metadata": {
        "id": "-c1azH0XC14R"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build a search index**"
      ],
      "metadata": {
        "id": "Kc17N9VeNHUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dimensions of the embeddings\n",
        "embeds = np.array(response)\n",
        "\n",
        "# Create the search index, pass the size of embedding\n",
        "search_index = AnnoyIndex(embeds.shape[1], 'angular')\n",
        "# Add all the vectors to the search index\n",
        "for i in range(len(embeds)):\n",
        "    search_index.add_item(i, embeds[i])\n",
        "\n",
        "search_index.build(10) # 10 trees\n",
        "search_index.save('test.ann')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FUv33sEvDIUg",
        "outputId": "a7e81abe-9144-4506-8266-6e59a4d50758"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Searching Article**"
      ],
      "metadata": {
        "id": "Dyjfc8Z7NP0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_document(query):\n",
        "    # Get the query's embedding\n",
        "    query_embed = co.embed(texts=[query]).embeddings\n",
        "\n",
        "    # Retrieve the nearest neighbors\n",
        "    similar_item_ids = search_index.get_nns_by_vector(query_embed[0],\n",
        "                                                    10,\n",
        "                                                  include_distances=True)\n",
        "\n",
        "    search_results = texts[similar_item_ids[0]]\n",
        "\n",
        "    return search_results"
      ],
      "metadata": {
        "id": "wZKnxmiRDWYD"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_document(\n",
        "    \"Which companies provided system-wide orchestration for generative AI?\"\n",
        ")\n",
        "\n",
        "print(results[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LDb3DG22DZUH",
        "outputId": "25feed54-a296-4143-fec7-434e1865660d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key Libraries & Frameworks\n",
            "Python-Based AI Development\n",
            "LangChain: Framework for chaining LLM calls and developing AI applications.\n",
            "Transformers (Hugging Face): Comprehensive library for working with pre-trained LLMs.\n",
            "PyTorch: Flexible and widely-used deep learning framework.\n",
            "TensorFlow: Google's machine learning platform for scalable model development.\n",
            "Model Deployment\n",
            "Ollama: Enables local model hosting and running.\n",
            "Hugging Face: Centralized platform for hosting and sharing models.\n",
            "MLflow: Streamlines the machine learning lifecycle, including tracking and deployment.\n",
            "Inference Platforms\n",
            "OpenAI API: Commercial access to advanced generative AI models.\n",
            "Groq: High-speed inference platform for large models.\n",
            "Anthropic API: Focused on ethical AI deployment.\n",
            "Google Vertex AI: Comprehensive AI deployment for enterprises.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating Answers**"
      ],
      "metadata": {
        "id": "G_SmbDhLNWiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question_about_document(question, num_generations=1):\n",
        "\n",
        "    # Search the text archive\n",
        "    results = search_document(question)\n",
        "\n",
        "    # Get the top result\n",
        "    context = results[0]\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt = f\"\"\"\n",
        "    overview of generative AI landscape:\n",
        "    {context}\n",
        "    Question: {question}\n",
        "\n",
        "    Extract the answer of the question from the text provided.\n",
        "    If the text doesn't contain the answer,\n",
        "    reply that the answer is not available.\"\"\"\n",
        "\n",
        "    prediction = co.generate(\n",
        "        prompt=prompt,\n",
        "        max_tokens=150,\n",
        "        model=\"command-nightly\",\n",
        "        temperature=0.5,\n",
        "        num_generations=num_generations\n",
        "    )\n",
        "\n",
        "    return prediction.generations"
      ],
      "metadata": {
        "id": "oFN33VcRDbSo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = ask_question_about_document(\n",
        "    \"Which companies provided system-wide orchestration for generative AI?\",\n",
        ")\n",
        "\n",
        "print(results[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fot_Y-rGDeaO",
        "outputId": "5fd4ea41-b4b9-49b8-aa09-3375f412d40b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer is not available. The text provided does not mention any companies offering system-wide orchestration for generative AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = ask_question_about_document(\n",
        "    \"Are side projects a good idea when trying to build a career in AI?\",\n",
        "    num_generations=2\n",
        ")\n",
        "\n",
        "for gen in results:\n",
        "    print(gen.text)\n",
        "    print('--')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2UMePqNRDhPk",
        "outputId": "6bd86b29-5c8d-4f32-e821-a6bc2e2d356d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text provided focuses on the practical application domains of Generative AI and does not specifically mention side projects. Therefore, the answer to your question is not directly available in the text.\n",
            "--\n",
            "The text provided focuses on the practical application domains of Generative AI and does not discuss the topic of side projects in relation to building a career in AI. Therefore, the answer is not available in the given text.\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = ask_question_about_document(\n",
        "    \"What are the fundamental tools to know in order to get started with generative AI?\",\n",
        "    num_generations=1\n",
        ")"
      ],
      "metadata": {
        "id": "x2_mGKP5DoVE"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for gen in results:\n",
        "    print(gen.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1aTCSVzsDrBr",
        "outputId": "2b014b43-f04e-4650-d324-05e0f409d098"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text provides a comprehensive overview of the Generative AI landscape, but it does not specify the fundamental tools required to get started. Therefore, the answer is not directly available in the text provided.\n"
          ]
        }
      ]
    }
  ]
}